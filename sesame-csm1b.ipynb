{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fdf7dad2-f889-4dad-b292-0c182cad1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from scipy.stats import pearsonr\n",
    "import parselmouth\n",
    "\n",
    "def load_wav(path, target_sr=None, mono=True):\n",
    "    \"\"\"WAV 파일을 로드하고 필요시 리샘플링합니다.\"\"\"\n",
    "    y, sr = sf.read(path)\n",
    "    if y.ndim > 1 and mono:\n",
    "        y = np.mean(y, axis=1)\n",
    "    if (target_sr is not None) and (sr != target_sr):\n",
    "        y = librosa.resample(y.astype(np.float32), orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "    return y.astype(np.float32), sr\n",
    "\n",
    "def _extract_f0_parselmouth(y, sr, hop_ms=10, fmin=50, fmax=600):\n",
    "    \"\"\"Praat(Parselmouth)로 F0 추출. 무성 구간은 np.nan.\"\"\"\n",
    "    snd = parselmouth.Sound(y, sampling_frequency=sr)\n",
    "    time_step = hop_ms / 1000.0\n",
    "    pitch = parselmouth.praat.call(\n",
    "        snd, \"To Pitch\", time_step, fmin,  # time step, min pitch\n",
    "        0.03, \"cc\", 0.25, 0.01, fmax      # 기타 디폴트 파라미터 + max pitch\n",
    "    )\n",
    "    n_frames = pitch.get_number_of_frames()\n",
    "    times = np.array([pitch.get_time_from_frame_number(i+1) for i in range(n_frames)], dtype=np.float32)\n",
    "    f0 = np.array([pitch.get_value_in_frame(i+1) for i in range(n_frames)], dtype=np.float32)\n",
    "    # 무성(unvoiced)은 <= 0으로 반환될 수 있으므로 nan으로 치환\n",
    "    f0[f0 <= 0] = np.nan\n",
    "    return times, f0\n",
    "\n",
    "\n",
    "def _extract_f0_pyinn(y, sr, hop_ms=10, fmin=50, fmax=600):\n",
    "    \"\"\"librosa.pyin으로 F0 추출. 무성 구간은 np.nan.\"\"\"\n",
    "    frame_length = 2048\n",
    "    hop_length = max(1, int(sr * (hop_ms / 1000.0)))\n",
    "    f0, _, _ = librosa.pyin(\n",
    "        y, fmin=fmin, fmax=fmax, sr=sr,\n",
    "        frame_length=frame_length, hop_length=hop_length\n",
    "    )\n",
    "    times = librosa.frames_to_time(np.arange(len(f0)), sr=sr, hop_length=hop_length)\n",
    "    return times.astype(np.float32), f0.astype(np.float32)\n",
    "\n",
    "def extract_f0(y, sr, hop_ms=10, fmin=50, fmax=600):\n",
    "    \"\"\"가능하면 Praat, 아니면 pyin으로 F0를 추출.\"\"\"\n",
    "    if _HAS_PARSELMOUTH:\n",
    "        return _extract_f0_parselmouth(y, sr, hop_ms=hop_ms, fmin=fmin, fmax=fmax)\n",
    "    else:\n",
    "        return _extract_f0_pyinn(y, sr, hop_ms=hop_ms, fmin=fmin, fmax=fmax)\n",
    "\n",
    "def _nearest_frame_values(times_src, vals_src, times_tgt):\n",
    "    \"\"\"\n",
    "    src의 시간축(times_src)에서 tgt 시간축(times_tgt)에 가장 가까운 프레임 값을 뽑는다.\n",
    "    (보간 대신 최근접 매핑 사용; 원본의 무성은 그대로 nan 유지)\n",
    "    \"\"\"\n",
    "    idxs = np.searchsorted(times_src, times_tgt, side=\"left\")\n",
    "    idxs = np.clip(idxs, 0, len(times_src)-1)\n",
    "    # 좌우 중 더 가까운 쪽으로 한 번 더 보정\n",
    "    left = np.clip(idxs - 1, 0, len(times_src)-1)\n",
    "    right = idxs\n",
    "    choose_left = np.abs(times_src[left] - times_tgt) <= np.abs(times_src[right] - times_tgt)\n",
    "    nearest = np.where(choose_left, left, right)\n",
    "    return vals_src[nearest]\n",
    "\n",
    "\n",
    "def _make_common_time_grid(t1, t2, hop_ms=10):\n",
    "    \"\"\"두 시퀀스의 공통 시간 격자를 만든다(겹치는 구간만).\"\"\"\n",
    "    start = max(float(np.min(t1)), float(np.min(t2)))\n",
    "    end = min(float(np.max(t1)), float(np.max(t2)))\n",
    "    if end <= start:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    step = hop_ms / 1000.0\n",
    "    # 끝점을 포함하도록 약간의 여유\n",
    "    grid = np.arange(start, end + 1e-6, step, dtype=np.float32)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def compute_f0_corr(\n",
    "    y_ref, sr_ref, y_gen, sr_gen,\n",
    "    hop_ms=10, fmin=50, fmax=600,\n",
    "    log_scale=True\n",
    "):\n",
    "    \"\"\"\n",
    "    레퍼런스/생성 음성의 F0 상관계수를 계산.\n",
    "    - Praat(또는 pyin)으로 F0 추출\n",
    "    - 공통 시간 격자에 최근접 매핑\n",
    "    - 양쪽 모두 유성(비-nan) 프레임만 Pearson r\n",
    "    반환값: dict(pearson_r, n_frames_used, voiced_rate_ref, voiced_rate_gen, hop_ms, fmin, fmax, log_scale)\n",
    "    \"\"\"\n",
    "    # 1) F0 추출\n",
    "    t_ref, f0_ref = extract_f0(y_ref, sr_ref, hop_ms=hop_ms, fmin=fmin, fmax=fmax)\n",
    "    t_gen, f0_gen = extract_f0(y_gen, sr_gen, hop_ms=hop_ms, fmin=fmin, fmax=fmax)\n",
    "\n",
    "    if len(t_ref) < 2 or len(t_gen) < 2:\n",
    "        return {\n",
    "            \"pearson_r\": np.nan,\n",
    "            \"n_frames_used\": 0,\n",
    "            \"voiced_rate_ref\": 0.0,\n",
    "            \"voiced_rate_gen\": 0.0,\n",
    "            \"hop_ms\": hop_ms, \"fmin\": fmin, \"fmax\": fmax, \"log_scale\": log_scale\n",
    "        }\n",
    "\n",
    "    # 2) 공통 시간 격자 생성\n",
    "    grid = _make_common_time_grid(t_ref, t_gen, hop_ms=hop_ms)\n",
    "    if len(grid) < 3:\n",
    "        return {\n",
    "            \"pearson_r\": np.nan,\n",
    "            \"n_frames_used\": 0,\n",
    "            \"voiced_rate_ref\": float(np.mean(np.isfinite(f0_ref))),\n",
    "            \"voiced_rate_gen\": float(np.mean(np.isfinite(f0_gen))),\n",
    "            \"hop_ms\": hop_ms, \"fmin\": fmin, \"fmax\": fmax, \"log_scale\": log_scale\n",
    "        }\n",
    "\n",
    "    # 3) 최근접 매핑으로 각각의 격자 F0 획득\n",
    "    f0g_ref = _nearest_frame_values(t_ref, f0_ref, grid)\n",
    "    f0g_gen = _nearest_frame_values(t_gen, f0_gen, grid)\n",
    "\n",
    "    # 4) 유성(둘 다 유효) 마스크\n",
    "    mask = np.isfinite(f0g_ref) & np.isfinite(f0g_gen)\n",
    "\n",
    "    if np.count_nonzero(mask) < 3:\n",
    "        return {\n",
    "            \"pearson_r\": np.nan,\n",
    "            \"n_frames_used\": int(np.count_nonzero(mask)),\n",
    "            \"voiced_rate_ref\": float(np.mean(np.isfinite(f0_ref))),\n",
    "            \"voiced_rate_gen\": float(np.mean(np.isfinite(f0_gen))),\n",
    "            \"hop_ms\": hop_ms, \"fmin\": fmin, \"fmax\": fmax, \"log_scale\": log_scale\n",
    "        }\n",
    "\n",
    "    x = f0g_ref[mask]\n",
    "    y = f0g_gen[mask]\n",
    "\n",
    "    if log_scale:\n",
    "        # log-F0로 상관 (음높이 지각과 더 잘 일치)\n",
    "        x = np.log(x)\n",
    "        y = np.log(y)\n",
    "\n",
    "    r, _ = pearsonr(x, y)\n",
    "\n",
    "    return {\n",
    "        \"pearson_r\": float(r),\n",
    "        \"n_frames_used\": int(len(x)),\n",
    "        \"voiced_rate_ref\": float(np.mean(np.isfinite(f0_ref))),\n",
    "        \"voiced_rate_gen\": float(np.mean(np.isfinite(f0_gen))),\n",
    "        \"hop_ms\": hop_ms, \"fmin\": fmin, \"fmax\": fmax, \"log_scale\": log_scale\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0ffcc-8798-43b4-86dd-0ea4cbd01ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    주피터 셀에서 바로 실행 가능한 데모.\n",
    "    - REF_PATH: 레퍼런스(정답) WAV\n",
    "    - GEN_PATH: 생성(모델 출력) WAV\n",
    "    \"\"\"\n",
    "    # >>> 여기만 여러분 환경에 맞게 수정하세요 <<<\n",
    "    REF_PATH = \"data/LJ001-0004.wav\"\n",
    "    GEN_PATH = \"eval_output/LJ001-0004-gen.wav\"\n",
    "\n",
    "    y_ref, sr_ref = load_wav(REF_PATH)\n",
    "    y_gen, sr_gen = load_wav(GEN_PATH)\n",
    "\n",
    "    # F0 corr 계산\n",
    "    metrics = compute_f0_corr(\n",
    "        y_ref, sr_ref, y_gen, sr_gen,\n",
    "        hop_ms=10, fmin=50, fmax=600, log_scale=True\n",
    "    )\n",
    "\n",
    "    # 보기 좋게 출력\n",
    "    print(\"=== F0 Correlation Result ===\")\n",
    "    print(f\"Pearson r (log-F0): {metrics['pearson_r']:.4f}\" if np.isfinite(metrics['pearson_r']) else \"Pearson r: NaN\")\n",
    "    print(f\"Frames used (voiced&voiced): {metrics['n_frames_used']}\")\n",
    "    print(f\"Voiced rate (ref): {metrics['voiced_rate_ref']*100:.1f}%\")\n",
    "    print(f\"Voiced rate (gen): {metrics['voiced_rate_gen']*100:.1f}%\")\n",
    "    print(f\"Params: hop={metrics['hop_ms']}ms, f0_range=[{metrics['fmin']},{metrics['fmax']}], log_scale={metrics['log_scale']}\")\n",
    "\n",
    "\n",
    "# 주피터 셀에서 바로 실행되도록\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da57aa26-9818-4166-9c8f-52a4f64641ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CsmForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"sesame/csm-1b\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n",
    "\n",
    "text = \"[0]Hello from Sesame.\" \n",
    "inputs = processor(text, add_special_tokens=True).to(device)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"0\", \"content\": [{\"type\": \"text\", \"text\": \"hello sesame\"}]},\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    ").to(device)\n",
    "\n",
    "# infer the model\n",
    "audio = model.generate(**inputs, output_audio=True)\n",
    "processor.save_audio(audio, \"example_without_context.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa366ae0-7764-47e5-86db-fd4a70a9efa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'sesame-csm1b' did not match any files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!git add sesame-csm1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9b617-e4f2-4678-b0f1-ae43749162d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
